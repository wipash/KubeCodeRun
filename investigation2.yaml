title: "Socket Hang-Up Investigation - Phase 2"
date: "2026-02-10"
branch: hangup-fixes
version_tested: 2.1.3.dev2

summary: |
  Continuing investigation of persistent "socket hang up" errors when LibreChat
  calls KubeCodeRun POST /exec with large files (~46MB CSV). Small files and
  simple code executions work reliably. Three fixes from Phase 1 have been
  deployed but the problem persists.

fixes_applied:
  - commit: a5322f5
    description: "Replace BaseHTTPMiddleware with pure ASGI for MetricsMiddleware"
    rationale: "BaseHTTPMiddleware uses background task + MemoryObjectStream that can silently fail for long-running requests"
    status: deployed

  - commit: b21969c
    description: "Increase uvicorn keep-alive timeout from 5s to 75s"
    rationale: "Default 5s keep-alive could close idle connections between requests"
    status: deployed

  - commit: earlier
    description: "Fire-and-forget cleanup via asyncio.create_task"
    rationale: "Blocking cleanup delayed response delivery"
    status: deployed

smoking_gun:
  description: |
    In fail2.log, POST /exec requests complete through ALL middleware layers
    (status 200 logged by RequestLoggingMiddleware) but have NO uvicorn access log.
    GET /files requests on the same pod DO have uvicorn access logs.
    This means responses reach the middleware output but uvicorn fails to write
    them to the TCP socket.
  evidence:
    - "fail2.log line 40: middleware logs 'Request processed' status=200 for POST /exec"
    - "fail2.log: NO 'INFO: 10.x.x.x:port - POST /exec HTTP/1.1 200 OK' line appears"
    - "fail2.log lines 15,17,49: GET /files DO have uvicorn access logs"
    - "success1.log: ALL POST /exec requests have BOTH middleware AND uvicorn access logs"

timestamp_analysis:
  librechat_error_time: "16:47:00 NZDT (03:47:00 UTC)"
  kcr_middleware_completion: "03:47:02 UTC"
  kcr_request_received: "03:46:55 UTC"
  gap: |
    LibreChat sees socket hang-up ~2 seconds BEFORE KCR finishes processing.
    The TCP connection closes approximately 5 seconds into request processing
    (03:47:00 - 03:46:55 = 5s). KCR continues processing unaware of the
    closed connection and the response is silently dropped by uvicorn's
    httptools transport (transport.is_closing() returns True, write is skipped).

request_flow_timing:
  - time: "03:46:55.709"
    event: "POST /exec received by KCR"
  - time: "~03:46:55.711"
    event: "Session lookup from file reference"
  - time: "~03:46:56.0"
    event: "File download from MinIO (~46MB) into memory"
  - time: "03:46:56.169"
    event: "Starting code execution (file upload to sidecar begins)"
  - time: "~03:46:56-03:47:00"
    event: "46MB file upload to sidecar pod via httpx POST /files"
  - time: "~03:47:00"
    event: "TCP CONNECTION CLOSED (by unknown cause)"
  - time: "~03:47:00-03:47:02"
    event: "Code executes in sidecar (nsenter subprocess)"
  - time: "03:47:02.438"
    event: "Execution completed, response ready"
  - time: "03:47:02.439"
    event: "Middleware logs status=200, but uvicorn silently drops response"

key_question: |
  What closes the LibreChat→KCR TCP connection at ~03:47:00 UTC,
  5 seconds into request processing while KCR is uploading a 46MB file
  to the sidecar pod?

theories:
  - name: "uvicorn httptools transport closing"
    confidence: high
    description: |
      uvicorn's httptools protocol silently drops response messages when
      transport.is_closing() is True. This explains why middleware sees
      status=200 but no uvicorn access log appears. However, this is the
      MECHANISM of failure, not the ROOT CAUSE of why the transport closes.

  - name: "Kubernetes networking / conntrack"
    confidence: low
    description: |
      conntrack TCP established timeout is typically 432000s (5 days).
      ClusterIP uses kube-proxy iptables rules which don't add extra timeouts.
      5 seconds is far too short for any standard K8s networking timeout.

  - name: "LibreChat-side timeout at agent framework level"
    confidence: medium
    description: |
      While node-fetch itself has no timeout, there may be a timeout at the
      LibreChat agent/tool execution framework level that aborts the request.
      However, the user pointed out that "socket hang up" means the SERVER
      closed the connection, not the client. If LibreChat aborted, we'd see
      "request aborted" or "fetch failed", not "socket hang up".

  - name: "uvicorn h11/httptools request processing bug"
    confidence: low
    description: |
      Possible bug in uvicorn or httptools that closes the transport under
      certain conditions (e.g., when event loop is busy with large async I/O).
      The event loop IS busy during the 46MB file upload to sidecar.

  - name: "Readiness probe failure removing Service endpoint"
    confidence: low
    description: |
      If readiness probe fails, the pod is removed from Service endpoints.
      However: (1) this only affects NEW connections, not existing ones,
      (2) it requires 3 failures at 10s intervals = 30s, not 5s,
      (3) health endpoint is fast and unlikely to fail.

  - name: "TCP RST from server pod (kernel or uvicorn)"
    confidence: medium
    description: |
      Something causes the KCR pod to send TCP RST or FIN to LibreChat.
      This could be: (a) uvicorn closing idle connections for some reason,
      (b) kernel TCP stack issue, (c) resource exhaustion. Would explain
      "socket hang up" (server-initiated close) rather than timeout.

diagnostic_logging_added:
  commit: 9d665ab
  description: "Diagnostic logging to identify exactly where responses are lost"
  changes:
    - file: src/middleware/metrics.py
      additions:
        - "receive_wrapper: detects http.disconnect events from client"
        - "send_wrapper: tracks response_body_sent flag, catches send() exceptions"
        - "finally block: warns if headers sent but body not confirmed"
        - "Passes receive_wrapper to inner app for disconnect monitoring"

    - file: src/api/exec.py
      additions:
        - "Checks http_request.is_disconnected() after orchestrator completes"
        - "Logs warning if client disconnected before response could be sent"
        - "Adds client_disconnected field to completion log"

expected_diagnostic_output:
  scenario_a:
    name: "Client disconnects first"
    expected_logs:
      - "'Client disconnected during request processing' with elapsed_ms"
      - "'Client disconnected before response could be sent' in exec.py"
      - "'Response headers sent but body not confirmed' (if headers were sent before disconnect)"
    interpretation: "LibreChat or something upstream is closing the connection"

  scenario_b:
    name: "send() fails with exception"
    expected_logs:
      - "'Failed to send response to client' with error details"
    interpretation: "uvicorn or transport layer actively rejects the response write"

  scenario_c:
    name: "Silent drop (no error, body not confirmed)"
    expected_logs:
      - "'Response headers sent but body not confirmed' WITHOUT send error"
      - "No 'Client disconnected' log"
    interpretation: "Transport closed silently, likely httptools transport.is_closing()"

next_steps:
  - priority: 1
    action: "Build and deploy image with diagnostic logging (commit 9d665ab)"
    purpose: "Collect diagnostic data during reproduction with large file"

  - priority: 2
    action: "Test with curl directly bypassing LibreChat"
    command: |
      # From within the cluster or port-forward
      curl -X POST http://kubecoderun.default.svc.cluster.local:8000/exec \
        -H 'Content-Type: application/json' \
        -H 'X-API-Key: <key>' \
        -d '{"code":"import pandas as pd; df=pd.read_csv(\"/mnt/data/file.csv\"); print(df.shape)","lang":"py","files":[{"id":"<file_id>","session_id":"<session_id>","name":"file.csv"}]}'
    purpose: "Isolate whether LibreChat or the raw connection is the issue"

  - priority: 3
    action: "Check kubectl events and describe pod during reproduction"
    command: "kubectl get events --field-selector involvedObject.name=kubecoderun-c685874dc-p4n8r -w"
    purpose: "Look for OOM kills, probe failures, or endpoint changes"

  - priority: 4
    action: "Capture TCP traffic during reproduction"
    command: "kubectl exec -it <pod> -- tcpdump -i eth0 port 8000 -w /tmp/capture.pcap"
    purpose: "See exactly who sends TCP FIN/RST and when"

  - priority: 5
    action: "Investigate LibreChat CodeExecutor.ts for any hidden timeout"
    purpose: "Despite node-fetch having no timeout, the agent framework may abort"

networking:
  path: "LibreChat pod → kube-proxy/iptables → KCR ClusterIP Service → KCR pod"
  notes:
    - "No ingress/proxy in path (direct ClusterIP)"
    - "No service mesh"
    - "LibreChat BASEURL: http://kubecoderun.default.svc.cluster.local:8000"
    - "Client IP in logs: 10.244.0.244 (LibreChat pod)"

files_examined:
  - src/api/exec.py
  - src/middleware/metrics.py
  - src/middleware/security.py
  - src/main.py
  - src/services/orchestrator.py
  - src/services/execution/runner.py
  - src/services/kubernetes/manager.py
  - src/services/kubernetes/pool.py
  - src/config/__init__.py
  - src/models/exec.py
  - docker/api/Dockerfile
  - helm-deployments/kubecoderun/values.yaml
  - helm-deployments/kubecoderun/templates/configmap.yaml
  - homelab helmrelease.yaml
  - homelab externalsecret.yaml
  - fail2.log
  - success1.log
  - investigation.md

dependencies:
  uvicorn: "0.40.0"
  starlette: "0.50.0"
  httptools: "0.7.1"
  h11: "0.16.0"
  httpx: "used for sidecar communication"

reproduction:
  fails_with: "46MB CSV file, Python code (pandas read_csv)"
  succeeds_with: "Small files, simple code, rapid sequential calls"
  pool_config: "poolSize=2 for Python"
  execution_time: "~6.7s for failing case, ~19-20s for success case (job fallback)"
